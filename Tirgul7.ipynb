{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voQt1cu0ce-h",
        "outputId": "6749d1e9-f61a-47aa-c847-f873235818cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page(url):\n",
        " response = requests.get(url)\n",
        " if response.status_code == 200:\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  return soup\n",
        " else:\n",
        "  return None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def index_words(soup):\n",
        "  index = {}\n",
        "  words = re.findall(r'\\w+', soup.get_text())\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word in index:\n",
        "      index[word] += 1\n",
        "    else:\n",
        "      index[word] = 1\n",
        "  return index"
      ],
      "metadata": {
        "id": "z_oG5HYzc0dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(index):\n",
        "  stop_words = {'a', 'an', 'the', 'and', 'or',     \t\t\t\t'in', 'on', 'at'}\n",
        "  for stop_word in stop_words:\n",
        "    if stop_word in index:\n",
        "      del index[stop_word]\n",
        "  return index"
      ],
      "metadata": {
        "id": "6Qb9vL0rdPyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def apply_stemming(index):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_index = {}\n",
        "  for word, count in index.items():\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if stemmed_word in stemmed_index:\n",
        "      stemmed_index[stemmed_word] += count\n",
        "    else:\n",
        "      stemmed_index[stemmed_word] = count\n",
        "  return stemmed_index"
      ],
      "metadata": {
        "id": "hKG3m2TOdYTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results"
      ],
      "metadata": {
        "id": "qYFBlAu5dpQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine(url, query):\n",
        "  soup = fetch_page(url)\n",
        "  if soup is None:\n",
        "     return None\n",
        "  index = index_words(soup)\n",
        "  index = remove_stop_words(index)\n",
        "  index = apply_stemming(index)\n",
        "  results = search(query, index)\n",
        "  return results"
      ],
      "metadata": {
        "id": "UJ7bA3_bd302"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'bird'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cm584qoeFlz",
        "outputId": "f888e1e7-4c0a-4bf8-f659-efef8d7d2fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bird': 574}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E3z42zqeZSQ",
        "outputId": "9b73e73e-ad57-4809-8183-599132b45c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "  stemmer = PorterStemmer()\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    word = stemmer.stem(word)\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results"
      ],
      "metadata": {
        "id": "lK_fHt8Aebwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUBSB66JfHQM",
        "outputId": "ec431cbb-c896-4882-8dae-0862dd65e8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bird': 574, 'wing': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n"
      ],
      "metadata": {
        "id": "izmK0W0TfT2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        "   rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrF-H-FCfcB2",
        "outputId": "0459d599-cbc6-4751-d791-ca4cc3c35e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bird': 574, 'wing': 25}\n",
            "0.9999303135888502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'collage students'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gCceVI1fhVt",
        "outputId": "b5747ff2-7f33-41fd-ab80-026869e7cd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'owls'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NGHh_Trfz1y",
        "outputId": "96ef4222-ee53-414e-860f-b17de1f98c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'owl': 13}\n",
            "0.9230769230769231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://w3.braude.ac.il/?lang=en'\n",
        "query = 'Industry'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AzhTcwKnuZI",
        "outputId": "1f1c6e29-c73f-4a3d-b72c-6ca732e9840d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'industri': 7}\n",
            "0.8571428571428572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### **מנוע המיועד למספר דפים**"
      ],
      "metadata": {
        "id": "NNHQfDngf-QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "class WikiSearchEngine:\n",
        "  def __init__(self):\n",
        "    \"\"\"Initialize the search engine\"\"\"\n",
        "    self.base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    self.pages = []\n",
        "    self.word_locations = defaultdict(list) # word -> [(page_id, frequency), ...]\n",
        "    self.stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}\n",
        "    return False\n",
        "  def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "    \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "    search_params = {\n",
        "      \"action\": \"query\",\n",
        "      \"format\": \"json\",\n",
        "      \"list\": \"search\",\n",
        "      \"srsearch\": topic,\n",
        "      \"srlimit\": num_pages\n",
        "    }\n",
        "    try:\n",
        "      response = requests.get(self.base_url, params=search_params)\n",
        "      search_results = response.json()['query']['search']\n",
        "\n",
        "      for result in search_results:\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"pageids\": result['pageid'],\n",
        "            \"inprop\": \"url\",\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "        content_response = requests.get(self.base_url, params=content_params)\n",
        "        page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "        self.pages.append({\n",
        "          'id': result['pageid'],\n",
        "          'title': page_data['title'],\n",
        "          'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "          'content': page_data['extract']\n",
        "        })\n",
        "      print(f\"Retrieved: {page_data['title']}\")\n",
        "      return True\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error fetching pages: {str(e)}\")\n",
        "\n",
        "  def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        # Process each page\n",
        "        for page in self.pages:\n",
        "            # Get all words from content\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            # Count word frequencies\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add to index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "  def search(self, query, num_results=5):\n",
        "        \"\"\"Search pages using simple word frequency ranking.\n",
        "        Ranks pages based on:1. Number of query words found in the page\n",
        "        2. Total frequency of query words  \"\"\"\n",
        "        # Get query words\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                    if word.lower() not in self.stop_words]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Calculate scores for each page\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        # For each query word\n",
        "        for word in query_words:\n",
        "            # Find pages containing this word\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "\n",
        "        # Convert to list and sort\n",
        "        ranked_results = [\n",
        "            (page_id, scores['matches'], scores['total_freq'])\n",
        "            for page_id, scores in page_scores.items()\n",
        "        ]\n",
        "        # Sort by number of matching words first, then by total frequency\n",
        "        ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        # Format results\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            # Find the first matching word context\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "gK6qrTDSf91I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "  \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "  search_params = {\n",
        "  \"action\": \"query\",\n",
        "  \"format\": \"json\",\n",
        "  \"list\": \"search\",\n",
        "  \"srsearch\": topic,\n",
        "  \"srlimit\": num_pages\n",
        "  }\n",
        "  try:\n",
        "      response = requests.get(self.base_url, params=search_params)\n",
        "      search_results = response.json()['query']['search']\n",
        "\n",
        "      for result in search_results:\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"pageids\": result['pageid'],\n",
        "            \"inprop\": \"url\",\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "        content_response = requests.get(self.base_url, params=content_params)\n",
        "        page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "        self.pages.append({\n",
        "          'id': result['pageid'],\n",
        "          'title': page_data['title'],\n",
        "          'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "          'content': page_data['extract']\n",
        "        })\n",
        "        print(f\"Retrieved: {page_data['title']}\")\n",
        "      return True\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching pages: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "XvdwliQ7hEhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        # Process each page\n",
        "        for page in self.pages:\n",
        "            # Get all words from content\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            # Count word frequencies\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add to index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "    def search(self, query, num_results=5):\n",
        "        \"\"\"Search pages using simple word frequency ranking.\n",
        "        Ranks pages based on:1. Number of query words found in the page\n",
        "        2. Total frequency of query words  \"\"\"\n",
        "        # Get query words\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                      if word.lower() not in self.stop_words]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Calculate scores for each page\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        # For each query word\n",
        "        for word in query_words:\n",
        "            # Find pages containing this word\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "\n",
        "        # Convert to list and sort\n",
        "        ranked_results = [\n",
        "            (page_id, scores['matches'], scores['total_freq'])\n",
        "            for page_id, scores in page_scores.items()\n",
        "        ]\n",
        "        # Sort by number of matching words first, then by total frequency\n",
        "        ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        # Format results\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            # Find the first matching word context\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results"
      ],
      "metadata": {
        "id": "F-5SwSt6jv3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}